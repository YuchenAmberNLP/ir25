{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (0.32.5)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ir_multilingual_py310/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: 2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS.\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Auto Device Selection\n",
    "import torch\n",
    "\n",
    "def get_best_device():\n",
    "    \"\"\"\n",
    "    Automatically detect the best available device: CUDA > MPS > CPU.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA GPU.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        print(\"Using Apple MPS.\")\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_best_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 complete: Loaded 100 Chinese news documents into variable 'docs'.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the first Demo 100 Chinese news documents from TREC NeuCLIR1 using streaming mode\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Streaming mode: only download and iterate over needed samples\n",
    "streamed = load_dataset('neuclir/neuclir1', split='zho', streaming=True)\n",
    "docs = []\n",
    "for i, item in enumerate(streamed):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    docs.append({\n",
    "        \"doc_id\": item[\"id\"],\n",
    "        \"text\": f\"{item['title']} {item['text']}\"\n",
    "    })\n",
    "print(\"Step 1 complete: Loaded 100 Chinese news documents into variable 'docs'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected queries for demo:\n",
      "{'query_id': 300, 'title': '“日本自杀率 COVID-19', 'description': 'COVID-19 大流行对日本自杀率有何影响？', 'narrative': '查找有关冠状病毒大流行对日本自杀率影响的文章和报告。如果文章提及在 COVID-19 大流行期间其他次要原因，也可归为是相关的。只谈论日本高自杀率及其原因而不提及 COVID-19 大流行的文章是不相关的。与其他国家的 COVID-19 大流行期间自杀率相关的文章是无关紧要的。'}\n",
      "{'query_id': 300, 'title': '日本新冠肺炎 (COVID-19) 自杀率\\n', 'description': 'COVID-19 大流行对日本自杀率有何影响？', 'narrative': '查找有关冠状病毒大流行对日本自杀率影响的文章和报告。如果在 COVID-19 大流行期间提到其他次要原因，它们也被认为是相关的。只谈论日本高自杀率及其原因而不提及 COVID-19 大流行的文章是不相关的。与其他国家的 COVID-19 大流行中自杀流行率相关的文章是无关紧要的。'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load official NeuCLIR 2024 queries for Chinese (or other language as needed)\n",
    "import json\n",
    "\n",
    "QUERYS_FILE = \"/Users/lianqi/Desktop/TODOS/IR/Project_multiretrieval/neuclir24.topics.0614.jsonl.txt\"\n",
    "# Load all queries from NeuCLIR 2024 topics file\n",
    "queries_zh = []  # Chinese\n",
    "#queries_fa = []  # Persian\n",
    "#queries_ru = []  # Russian\n",
    "\n",
    "with open(QUERYS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        q_big = json.loads(line)\n",
    "        # q_big[\"topics\"] 是一个列表，遍历里面每个 topic\n",
    "        for q in q_big.get(\"topics\", []):\n",
    "            # 中文\n",
    "            if q[\"lang\"] == \"zho\":\n",
    "                queries_zh.append({\n",
    "                    \"query_id\": q_big[\"topic_id\"],  # 用顶层 topic_id 作为 query_id\n",
    "                    \"title\": q[\"topic_title\"],\n",
    "                    \"description\": q[\"topic_description\"],\n",
    "                    \"narrative\": q[\"topic_narrative\"]\n",
    "                })\n",
    "\"\"\"\n",
    "            # 波斯语\n",
    "            if q[\"lang\"] == \"fas\":\n",
    "                queries_fa.append({\n",
    "                    \"query_id\": q_big[\"topic_id\"],\n",
    "                    \"title\": q[\"topic_title\"],\n",
    "                    \"description\": q[\"topic_description\"],\n",
    "                    \"narrative\": q[\"topic_narrative\"]\n",
    "                })\n",
    "            # 俄语\n",
    "            if q[\"lang\"] == \"rus\":\n",
    "                queries_ru.append({\n",
    "                    \"query_id\": q_big[\"topic_id\"],\n",
    "                    \"title\": q[\"topic_title\"],\n",
    "                    \"description\": q[\"topic_description\"],\n",
    "                    \"narrative\": q[\"topic_narrative\"]\n",
    "                })\n",
    "\n",
    "\"\"\"\n",
    "# For demo, just use first 2 Chinese queries for a quick test\n",
    "queries = queries_zh[:2]\n",
    "# queries = queries_fa[:2] # Persian (uncomment to use)\n",
    "# queries = queries_ru[:2] # Russian (uncomment to use)\n",
    "\n",
    "print(\"Selected queries for demo:\")\n",
    "for q in queries:\n",
    "    print(q)\n",
    "\n",
    "# Example: use queries_zh for Chinese, queries_fa for Persian, queries_ru for Russian\n",
    "# To use a different language, just replace 'queries_zh' with your target, e.g.:\n",
    "#   queries = queries_zh      # For Chinese document retrieval\n",
    "#   queries = queries_fa      # For Persian document retrieval\n",
    "#   queries = queries_ru      # For Russian document retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 complete: Models and IDF dictionary ready.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare for mBERT, m3, MGTE\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "\n",
    "# Model info: (name, loading_type)\n",
    "model_info = [\n",
    "    (\"bert-base-multilingual-cased\", \"hf\"),        # mBERT\n",
    "    (\"bert-base-multilingual-uncased\", \"hf\"),      # m3\n",
    "    (\"intfloat/multilingual-e5-large-instruct\", \"st\")  # MGTE\n",
    "]\n",
    "\n",
    "def encode_with_hf(model_name, texts, return_token_embs=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        # Mean pooling for sentence embedding\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        embeddings = sum_embeddings / sum_mask\n",
    "        if return_token_embs:\n",
    "            # Get all valid token embeddings (remove padding)\n",
    "            token_embeddings = []\n",
    "            for i in range(last_hidden.size(0)):\n",
    "                mask = attention_mask[i].bool()\n",
    "                valid_tokens = last_hidden[i][mask]\n",
    "                token_embeddings.append(valid_tokens)\n",
    "            return embeddings, token_embeddings\n",
    "        else:\n",
    "            return embeddings\n",
    "\n",
    "def encode_with_st(model_name, texts, return_token_embs=False):\n",
    "    model = SentenceTransformer(model_name, device=str(DEVICE))\n",
    "    if return_token_embs:\n",
    "        tokenizer = model.tokenizer\n",
    "        transformer = model._first_module().auto_model\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            outputs = transformer(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            token_embeddings = []\n",
    "            for i in range(last_hidden.size(0)):\n",
    "                mask = attention_mask[i].bool()\n",
    "                valid_tokens = last_hidden[i][mask]\n",
    "                token_embeddings.append(valid_tokens)\n",
    "            # Mean pooling for sentence embedding\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "            return embeddings, token_embeddings\n",
    "    else:\n",
    "        return model.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
    "\n",
    "\n",
    "# Prepare doc texts and IDs\n",
    "doc_texts = [d[\"text\"] for d in docs]\n",
    "doc_ids = [d[\"doc_id\"] for d in docs]\n",
    "\n",
    "# Prepare TF-IDF for BWE-Agg-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(doc_texts + [f\"{q['title']} {q['description']} {q['narrative']}\" for q in queries])\n",
    "idf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "print(\"Step 3 complete: Models and IDF dictionary ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Using encoder: bert-base-multilingual-cased ===\n",
      "\n",
      "--- Query: “日本自杀率 COVID-19 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/8rcdwsh9189405xvrvg8r4840000gn/T/ipykernel_46314/1414956869.py:68: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  q_vec = (q_token_embs.T @ q_idfs_tensor).T / sum(q_idfs)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2118 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.5022\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "      Rank 2: DocID a27b31d8-ad61-4280-9ede-ea5ab3479a26, Score: 0.4940\n",
      "         Text snippet: 習近平與捷克總統通話 盼捷方更多人士「正確看待中國」 大陸國家主席習近平7月7日晚上與捷克總統澤曼（Milos Zema...\n",
      "      Rank 3: DocID de56ae79-6512-4b7d-9754-f6dfde5c5ad8, Score: 0.4930\n",
      "         Text snippet: 北市頻傳私打疫苗 柯文哲：求生是人的本能 台北市日前接連爆發好心肝診所、振興醫院私打疫苗事件，市長柯文哲今天表示，求生是...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 6a84f09d-88f5-4bf6-b6b7-e6a2da7a4af8, Score: 0.6981\n",
      "         Text snippet: 食物價格一路漲 超市開始囤貨求自保 芝加哥一家超市內，顧客在選購水果。(Getty Images)\n",
      "\n",
      "食物價格一路走高，...\n",
      "      Rank 2: DocID 4654911d-e8ef-49b7-b12f-1ae2ce837a67, Score: 0.6967\n",
      "         Text snippet: 吃素不能打疫苗？醫師們解答籲轉念 網讚：達賴喇嘛都打AZ了 隨著疫苗陸續抵台，政府逐漸開放多類族群接種疫苗，不過仍有民眾...\n",
      "      Rank 3: DocID e95d764e-0d76-40aa-8119-154f64ab9db7, Score: 0.6935\n",
      "         Text snippet: 柯文哲：很多商品都中國製 為何疫苗不能用中國代理的？ 新冠肺炎疫情在台灣近日逐漸趨緩，不過能夠有效遏止疫情的疫苗，在台灣...\n",
      "\n",
      "--- Query: 日本新冠肺炎 (COVID-19) 自杀率\n",
      " ---\n",
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.5166\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "      Rank 2: DocID 9015b757-de73-4e89-8c22-fb7ea5ca3b2b, Score: 0.5147\n",
      "         Text snippet: 柯文哲稱被騙去環南市場 莊人祥：有說開記者會 指揮中心與台北市政府2日為環南市場疫情舉行聯合記者會，台北市長柯文哲今天晚...\n",
      "      Rank 3: DocID a27b31d8-ad61-4280-9ede-ea5ab3479a26, Score: 0.5132\n",
      "         Text snippet: 習近平與捷克總統通話 盼捷方更多人士「正確看待中國」 大陸國家主席習近平7月7日晚上與捷克總統澤曼（Milos Zema...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 970b91aa-ebec-4f10-9844-ca8bfd18c1dc, Score: 0.7047\n",
      "         Text snippet: 跑步時胸痛，是心臟病？醫師揭曉「八大原因」 掃描二維碼，觀賞完整健康講座影片。\n",
      "\n",
      "世界上有數百萬的人患有冠狀動脈疾病。這...\n",
      "      Rank 2: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.6962\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "      Rank 3: DocID 6a84f09d-88f5-4bf6-b6b7-e6a2da7a4af8, Score: 0.6923\n",
      "         Text snippet: 食物價格一路漲 超市開始囤貨求自保 芝加哥一家超市內，顧客在選購水果。(Getty Images)\n",
      "\n",
      "食物價格一路走高，...\n",
      "\n",
      "\n",
      "=== Using encoder: bert-base-multilingual-uncased ===\n",
      "\n",
      "--- Query: “日本自杀率 COVID-19 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2102 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID de56ae79-6512-4b7d-9754-f6dfde5c5ad8, Score: 0.5855\n",
      "         Text snippet: 北市頻傳私打疫苗 柯文哲：求生是人的本能 台北市日前接連爆發好心肝診所、振興醫院私打疫苗事件，市長柯文哲今天表示，求生是...\n",
      "      Rank 2: DocID 799f1e12-cf9c-468a-90f7-01ef7c827554, Score: 0.5748\n",
      "         Text snippet: 台日友好是「善的循環」展現 謝長廷：會是世界和平的模範 日本贈我疫苗，表示是為回報台灣311地震的恩情。謝長廷感動表示，...\n",
      "      Rank 3: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.5703\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 95a18cdd-da7a-4815-a07d-7365d8c2e268, Score: 0.8669\n",
      "         Text snippet: 欧科云链链上大师重磅上线，一起来用“链上Bloomberg”听听行业脉搏跳动 Wednesday, 7 July 202...\n",
      "      Rank 2: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.8584\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "      Rank 3: DocID 21fa73c8-e8c5-461b-a567-c08f87cf978b, Score: 0.8568\n",
      "         Text snippet: 张汉晖大使出席庆祝中国共产党成立100周年暨《中俄睦邻友好合作条约》签署20周年专场音乐会--国际--人民网 人民网莫斯...\n",
      "\n",
      "--- Query: 日本新冠肺炎 (COVID-19) 自杀率\n",
      " ---\n",
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID de56ae79-6512-4b7d-9754-f6dfde5c5ad8, Score: 0.5985\n",
      "         Text snippet: 北市頻傳私打疫苗 柯文哲：求生是人的本能 台北市日前接連爆發好心肝診所、振興醫院私打疫苗事件，市長柯文哲今天表示，求生是...\n",
      "      Rank 2: DocID 799f1e12-cf9c-468a-90f7-01ef7c827554, Score: 0.5796\n",
      "         Text snippet: 台日友好是「善的循環」展現 謝長廷：會是世界和平的模範 日本贈我疫苗，表示是為回報台灣311地震的恩情。謝長廷感動表示，...\n",
      "      Rank 3: DocID 9015b757-de73-4e89-8c22-fb7ea5ca3b2b, Score: 0.5786\n",
      "         Text snippet: 柯文哲稱被騙去環南市場 莊人祥：有說開記者會 指揮中心與台北市政府2日為環南市場疫情舉行聯合記者會，台北市長柯文哲今天晚...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 95a18cdd-da7a-4815-a07d-7365d8c2e268, Score: 0.8703\n",
      "         Text snippet: 欧科云链链上大师重磅上线，一起来用“链上Bloomberg”听听行业脉搏跳动 Wednesday, 7 July 202...\n",
      "      Rank 2: DocID 21fa73c8-e8c5-461b-a567-c08f87cf978b, Score: 0.8572\n",
      "         Text snippet: 张汉晖大使出席庆祝中国共产党成立100周年暨《中俄睦邻友好合作条约》签署20周年专场音乐会--国际--人民网 人民网莫斯...\n",
      "      Rank 3: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.8528\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "\n",
      "\n",
      "=== Using encoder: intfloat/multilingual-e5-large-instruct ===\n",
      "\n",
      "--- Query: “日本自杀率 COVID-19 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1502 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID 5f486e3f-5e1d-4ba5-8a81-e2a31dccf2e1, Score: 0.8243\n",
      "         Text snippet: 日本政府確定方針！ 東京奧運將在「緊急事態」中開幕 ▲日本政府確定東京第四次宣布緊急事態宣言的方針，勢必影響奧運開幕式。...\n",
      "      Rank 2: DocID abc181f7-f049-441c-959c-5e70adb6ecb5, Score: 0.8228\n",
      "         Text snippet: 新加坡不承認科興疫苗 港議員憂旅遊氣泡計劃受影響｜即時新聞｜港澳｜on.cc東網 內地自主研發的科興疫苗不被新加坡衞生部...\n",
      "      Rank 3: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.8223\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 5f486e3f-5e1d-4ba5-8a81-e2a31dccf2e1, Score: 0.8395\n",
      "         Text snippet: 日本政府確定方針！ 東京奧運將在「緊急事態」中開幕 ▲日本政府確定東京第四次宣布緊急事態宣言的方針，勢必影響奧運開幕式。...\n",
      "      Rank 2: DocID aaf37a05-2522-4af8-93d7-255e9c7a78e7, Score: 0.8371\n",
      "         Text snippet: 疑與男友吵架想不開 女墜落6米深橋下送醫 一名50多歲女子疑似因為與男友發生爭執，今晚心情不好，從台中市東勢區永安橋墜下...\n",
      "      Rank 3: DocID abc181f7-f049-441c-959c-5e70adb6ecb5, Score: 0.8364\n",
      "         Text snippet: 新加坡不承認科興疫苗 港議員憂旅遊氣泡計劃受影響｜即時新聞｜港澳｜on.cc東網 內地自主研發的科興疫苗不被新加坡衞生部...\n",
      "\n",
      "--- Query: 日本新冠肺炎 (COVID-19) 自杀率\n",
      " ---\n",
      "\n",
      "   [maxSim] Top 3 results:\n",
      "      Rank 1: DocID 5f486e3f-5e1d-4ba5-8a81-e2a31dccf2e1, Score: 0.8275\n",
      "         Text snippet: 日本政府確定方針！ 東京奧運將在「緊急事態」中開幕 ▲日本政府確定東京第四次宣布緊急事態宣言的方針，勢必影響奧運開幕式。...\n",
      "      Rank 2: DocID abc181f7-f049-441c-959c-5e70adb6ecb5, Score: 0.8265\n",
      "         Text snippet: 新加坡不承認科興疫苗 港議員憂旅遊氣泡計劃受影響｜即時新聞｜港澳｜on.cc東網 內地自主研發的科興疫苗不被新加坡衞生部...\n",
      "      Rank 3: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.8245\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "\n",
      "   [BWE-Agg-IDF] Top 3 results:\n",
      "      Rank 1: DocID 5f486e3f-5e1d-4ba5-8a81-e2a31dccf2e1, Score: 0.8432\n",
      "         Text snippet: 日本政府確定方針！ 東京奧運將在「緊急事態」中開幕 ▲日本政府確定東京第四次宣布緊急事態宣言的方針，勢必影響奧運開幕式。...\n",
      "      Rank 2: DocID abc181f7-f049-441c-959c-5e70adb6ecb5, Score: 0.8422\n",
      "         Text snippet: 新加坡不承認科興疫苗 港議員憂旅遊氣泡計劃受影響｜即時新聞｜港澳｜on.cc東網 內地自主研發的科興疫苗不被新加坡衞生部...\n",
      "      Rank 3: DocID ddd54cb0-fe35-4a8f-8a90-bc58b34590d7, Score: 0.8391\n",
      "         Text snippet: 新加坡研究：疫苗對抗Delta變異株 保護力達69% 新加坡表示，當地一項研究顯示，接種疫苗對防止感染Delta變異株的...\n",
      "\n",
      "Demo complete! You now have maxSim and BWE-Agg-IDF retrieval results for all three encoders.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Retrieval loop with multiple encoders and strategies\n",
    "import numpy as np\n",
    "\n",
    "# --- Utility: Align tokenization for IDF and embeddings ---\n",
    "def get_idf_aligned(tokenizer, text, idf_dict):\n",
    "    \"\"\"\n",
    "    Given a tokenizer and text, return the list of IDF weights for the tokens,\n",
    "    using the same tokenization as for embedding extraction. \n",
    "    This ensures IDF weights and token embeddings are strictly aligned \n",
    "    to avoid shape mismatch.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Remove special tokens like [CLS], [SEP] if present\n",
    "    tokens = [t for t in tokens if not t.startswith('[')]\n",
    "    idfs = [idf_dict.get(t.lower(), 1.0) for t in tokens]\n",
    "    return idfs\n",
    "\n",
    "# --- Main retrieval loop ---\n",
    "\n",
    "for model_name, loader in model_info:\n",
    "    print(f\"\\n\\n=== Using encoder: {model_name} ===\")\n",
    "    if loader == \"st\":\n",
    "        model = SentenceTransformer(model_name, device=str(DEVICE))\n",
    "        tokenizer = model.tokenizer\n",
    "        doc_sent_embs, doc_token_embs = encode_with_st(model_name, doc_texts, return_token_embs=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        doc_sent_embs, doc_token_embs = encode_with_hf(model_name, doc_texts, return_token_embs=True)\n",
    "    \n",
    "    for query in queries:\n",
    "        q_text = f\"{query['title']} {query['description']} {query['narrative']}\"\n",
    "        if loader == \"st\":\n",
    "            q_sent_emb, q_token_embs = encode_with_st(model_name, [q_text], return_token_embs=True)\n",
    "            q_token_embs = q_token_embs[0]\n",
    "        else:\n",
    "            q_sent_emb, q_token_embs = encode_with_hf(model_name, [q_text], return_token_embs=True)\n",
    "            q_token_embs = q_token_embs[0]\n",
    "\n",
    "        print(f\"\\n--- Query: {query['title']} ---\")\n",
    "\n",
    "        # --- maxSim ---\n",
    "        maxsim_scores = []\n",
    "        for doc_toks in doc_token_embs:\n",
    "            sims = []\n",
    "            for q_tok_vec in q_token_embs:\n",
    "                sim = torch.nn.functional.cosine_similarity(q_tok_vec.unsqueeze(0), doc_toks)\n",
    "                sims.append(sim.max().item())\n",
    "            score = np.mean(sims)\n",
    "            maxsim_scores.append(score)\n",
    "        maxsim_scores = torch.tensor(maxsim_scores)\n",
    "        topk = torch.topk(maxsim_scores, 3)\n",
    "        print(f\"\\n   [maxSim] Top 3 results:\")\n",
    "        for rank, idx in enumerate(topk.indices, 1):\n",
    "            print(f\"      Rank {rank}: DocID {doc_ids[idx]}, Score: {maxsim_scores[idx]:.4f}\")\n",
    "            print(f\"         Text snippet: {doc_texts[idx][:60]}...\")\n",
    "\n",
    "        # --- BWE-Agg-IDF ---\n",
    "        # Use utility to ensure IDF and embedding alignment!\n",
    "        q_idfs = get_idf_aligned(tokenizer, q_text, idf_dict)\n",
    "        emb_num = q_token_embs.shape[0]\n",
    "        idf_num = len(q_idfs)\n",
    "        if emb_num > idf_num:\n",
    "            q_token_embs = q_token_embs[:idf_num]\n",
    "        elif idf_num > emb_num:\n",
    "            q_idfs = q_idfs[:emb_num]\n",
    "        if len(q_idfs) > 0:\n",
    "            q_idfs_tensor = torch.tensor(q_idfs, device=DEVICE)\n",
    "            q_vec = (q_token_embs.T @ q_idfs_tensor).T / sum(q_idfs)\n",
    "        else:\n",
    "            q_vec = q_token_embs.mean(0)\n",
    "\n",
    "        agg_idf_scores = []\n",
    "        for doc_idx, doc_toks in enumerate(doc_token_embs):\n",
    "            # Use the same alignment for doc tokens\n",
    "            doc_idfs = get_idf_aligned(tokenizer, doc_texts[doc_idx], idf_dict)\n",
    "            emb_num = doc_toks.shape[0]\n",
    "            idf_num = len(doc_idfs)\n",
    "            if emb_num > idf_num:\n",
    "                doc_toks = doc_toks[:idf_num]\n",
    "            elif idf_num > emb_num:\n",
    "                doc_idfs = doc_idfs[:emb_num]\n",
    "            if len(doc_idfs) > 0:\n",
    "                d_idfs_tensor = torch.tensor(doc_idfs, device=doc_toks.device)\n",
    "                doc_vec = (doc_toks.T @ d_idfs_tensor).T / sum(doc_idfs)\n",
    "            else:\n",
    "                doc_vec = doc_toks.mean(0)\n",
    "            score = torch.nn.functional.cosine_similarity(\n",
    "                q_vec.unsqueeze(0), doc_vec.unsqueeze(0)\n",
    "            ).item()\n",
    "            agg_idf_scores.append(score)\n",
    "        agg_idf_scores = torch.tensor(agg_idf_scores)\n",
    "        topk = torch.topk(agg_idf_scores, 3)\n",
    "        print(f\"\\n   [BWE-Agg-IDF] Top 3 results:\")\n",
    "        for rank, idx in enumerate(topk.indices, 1):\n",
    "            print(f\"      Rank {rank}: DocID {doc_ids[idx]}, Score: {agg_idf_scores[idx]:.4f}\")\n",
    "            print(f\"         Text snippet: {doc_texts[idx][:60]}...\")\n",
    "\n",
    "print(\"\\nDemo complete! You now have maxSim and BWE-Agg-IDF retrieval results for all three encoders.\")\n",
    "\n",
    "# model_info = [\n",
    "#     (\"bert-base-multilingual-cased\", \"hf\"),        # mBERT\n",
    "#     (\"bert-base-multilingual-uncased\", \"hf\"),      # m3\n",
    "#     (\"intfloat/multilingual-e5-large-instruct\", \"st\")  # MGTE\n",
    "# ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_multilingual_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
